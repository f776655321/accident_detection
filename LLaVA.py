from openai import OpenAI
import base64
import cv2
import numpy as np
import os
from PIL import Image
from cls_model.model import Resnet50
from dotenv import load_dotenv
import random
import torch
import json
import time
# import websockets
import asyncio
from transformers import AutoProcessor, LlavaForConditionalGeneration

load_dotenv(dotenv_path='.env')

def set_seed(seed: int):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)  # if using CUDA

# async def send_data(text):
#     uri = "ws://localhost:8080"
#     async with websockets.connect(uri) as websocket:
#         data = {"type": "text-input", "payload": text}
#         await websocket.send(json.dumps(data))
#         response = await websocket.recv()
#         print("Received:", response)

set_seed(42)

class VideoProcessor:
    def __init__(self, processer, LM, interval_sec=0.5, model=None):
        self.processer = processer
        self.LM = LM
        self.interval_sec = interval_sec
        self.cap = None
        self.fps = None
        self.frame_interval = None
        self.model = model
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def _init_video(self, video_path):
        self.cap = cv2.VideoCapture(video_path)
        if not self.cap.isOpened():
            raise ValueError("Error: Can't open video.")
        self.fps = self.cap.get(cv2.CAP_PROP_FPS)
        self.frame_interval = int(self.fps * self.interval_sec)

    def _encode_image(self, frame):
        _, buffer = cv2.imencode('.jpg', frame)
        return base64.b64encode(buffer).decode('utf-8')
    
    def predict(self, frame):
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image_pil = Image.fromarray(image_rgb)
        return self.model.predict(image_pil) == 0 # 0 for Accident, 1 for Non Accident

    def llm(self, frames):
        conversation = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            "ä»¥ä¸‹æ˜¯å¾äº¤é€šç›£è¦–å™¨ä¸­é€£çºŒæ“·å–çš„ 3 å¼µäº‹æ•…ç›¸é—œç•«é¢ï¼Œå·²ç¶“ç¢ºèªæœ‰äº‹æ•…ç™¼ç”Ÿï¼Œè«‹ç”¨ç°¡æ½”ä¸­æ–‡æè¿°ç•«é¢å…§å®¹ã€‚"
                            "å†æ¨¡ä»¿äº¤é€šå»£æ’­ï¼ˆå¦‚è­¦å»£ FMï¼‰å³æ™‚æ’­å ±çš„èªæ°£ï¼Œç”¨ç°¡æ½”ã€å°ˆæ¥­çš„ä¸­æ–‡çµåˆç•«é¢å…§å®¹ï¼Œé€²è¡Œæ’­å ±ã€‚"
                            "æ™‚é–“ï¼š7:30 AMã€åœ°é»ï¼šå°åŒ—å¸‚å¿ å­æ±è·¯ä¸‰æ®µã€‚"
                            "å°‡æ’­å ±å…§å®¹æ•˜è¿°åœ¨ã€Œ[æ’­å ±å…§å®¹]ï¼šã€å¾Œã€‚"
                        )
                    },
                    {"type": "image"},
                    {"type": "image"},
                    {"type": "image"},
                ]
            }
        ]
        prompt = self.processer.apply_chat_template(conversation, add_generation_prompt=True)

        pil_images = []
        for frame in frames:
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_images.append(Image.fromarray(rgb))

        # 4. æŠŠå½±åƒè·Ÿ prompt ä¸€èµ· feed é€² processorï¼Œå¾—åˆ°æ¨¡å‹çš„è¼¸å…¥ tensors
        #    é€™è£¡ return_tensors="pt" æœƒå›å‚³ä¸€å€‹ dict è£¡é¢æœ‰ input_ids, attention_mask, pixel_values ç­‰ tensor
        #    å†æŠŠå®ƒå€‘ç§»åˆ°åŒä¸€å€‹ deviceï¼ˆä¾‹å¦‚ GPUï¼‰ï¼Œä¸¦ä¸”æŒ‡å®šä»¥ float16 ç²¾åº¦ï¼ˆè‹¥ä½ çš„ GPU æ”¯æ´ï¼‰
        inputs = self.processer(
            images=pil_images,
            text=prompt,
            return_tensors="pt"
        ).to(self.device, torch.float16)

        # 5. å‘¼å« model.generate å–å¾— output token åºåˆ—
        #    é€™é‚Š max_new_tokens å¯ä»¥è‡ªå·±èª¿æ•´ï¼Œçœ‹è¦è®“æ¨¡å‹æœ€å¤šç”Ÿæˆå¤šå°‘å­—
        output_ids = self.LM.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False  # è‹¥è¦æŠ½æ¨£å¯ä»¥æ”¹æˆ True ä¸¦ä¸”åŠ ä¸Š temperature/top_p/top_k
        )

        # 6. ç”¨ processor.decode æŠŠ output_ids è½‰æˆä¸­æ–‡æ–‡å­—
        #    output_ids[0] ä»£è¡¨ç¬¬ä¸€æ¢å›æ‡‰ã€‚å¦‚æœè¦è·³éå‰é¢ special tokensï¼ˆä¾‹å¦‚ BOS/EOSï¼‰ï¼Œå¯ä»¥åƒç¯„ä¾‹ä¸€æ¨£æŠŠå‰å…©å€‹ token è·³éï¼š output_ids[0][2:]
        decoded = self.processer.decode(output_ids[0][2:], skip_special_tokens=True)

        # 7. æ“·å–ã€Œ[æ’­å ±å…§å®¹]ï¼šã€ä¹‹å¾Œçš„éƒ¨åˆ†
        if "[æ’­å ±å…§å®¹]ï¼š" in decoded:
            return decoded.split("[æ’­å ±å…§å®¹]ï¼š")[-1].strip()
        else:
            # å¦‚æœæ²’æœ‰æ¨™è¨˜ï¼Œå°±ç›´æ¥å›å‚³å…¨éƒ¨
            return decoded.strip()

    def process(self, video_path):
        self._init_video(video_path)
        frame_count = 0
        accident_frames = []  # ä¿å­˜é€£çºŒäº‹æ•… frame

        while True:
            ret, frame = self.cap.read()
            if not ret:
                break
            if frame_count % self.frame_interval == 0:
                if self.predict(frame):
                    
                    accident_frames.append(frame)  # ä¿å­˜ frameï¼ˆcopy é¿å…å¾ŒçºŒè¢«è¦†è“‹ï¼‰
                    if len(accident_frames) == 3:
                        print(f"ğŸš¨ é€£çºŒ 3 å€‹äº‹æ•… frameï¼ŒåŸ·è¡Œ GPT èªªæ˜")
                        selected = [accident_frames[0], accident_frames[1], accident_frames[2]]
                        save_dir = "frames"
                        os.makedirs(save_dir, exist_ok=True)

                        # cv2.imwrite(os.path.join(save_dir, "accident_0.jpg"), accident_frames[0])
                        # cv2.imwrite(os.path.join(save_dir, "accident_1.jpg"), accident_frames[1])
                        # cv2.imwrite(os.path.join(save_dir, "accident_2.jpg"), accident_frames[2])
                        start = time.time()
                        llm_text = self.llm(selected)
                        print(llm_text)
                        end = time.time()
                        print(f"LLM è™•ç†æ™‚é–“: {end - start:.2f} ç§’")
                        # asyncio.run(send_data(llm_text))
                        accident_frames.clear()  # é‡ç½®
                        break
                else:
                    accident_frames.clear()  # ä¸­æ–·é€£çºŒ â†’ æ¸…ç©º
            frame_count += 1

        self.cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    print("Establish Video Processor...")
    model_id = "llava-hf/llava-1.5-7b-hf"
    model = LlavaForConditionalGeneration.from_pretrained(
        model_id, 
        torch_dtype=torch.float16, 
        low_cpu_mem_usage=True, 
    ).to(0)

    processer = AutoProcessor.from_pretrained(model_id)
    v = VideoProcessor(processer=processer, LM=model, interval_sec=0.2, model=Resnet50('Resnet50.pth'))
    input("Press Enter to start processing...")
    v.process("video/5.mp4")